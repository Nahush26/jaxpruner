{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ScyDt14r8MLU"
      },
      "outputs": [],
      "source": [
        "#@title LICENSE\n",
        "# Licensed under the Apache License, Version 2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] ='0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-06-30 03:10:34.348079: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-06-30 03:10:35.664838: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import torch.utils.data as data\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-BS3RFRLRotm"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-research/jaxpruner/blob/main/colabs/mnist_pruning.ipynb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt0rL4ycp4ZB"
      },
      "source": [
        "## Imports / Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EdzHCJuop4ZB"
      },
      "outputs": [],
      "source": [
        "from absl import logging\n",
        "import flax\n",
        "import jax.numpy as jnp\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "from flax import linen as nn\n",
        "from flax.metrics import tensorboard\n",
        "from flax.training import train_state\n",
        "import jax\n",
        "import optax\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "# Hide any GPUs form TensorFlow. Otherwise TF might reserve memory and make\n",
        "# it unavailable to JAX.\n",
        "tf.config.experimental.set_visible_devices([], \"GPU\")\n",
        "\n",
        "\n",
        "logging.set_verbosity(logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AKD2Zjfv8YnK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/google-research/jaxpruner\n",
            "  Cloning https://github.com/google-research/jaxpruner to /tmp/pip-req-build-1gywb8jb\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/google-research/jaxpruner /tmp/pip-req-build-1gywb8jb\n",
            "  Resolved https://github.com/google-research/jaxpruner to commit d92a781c7e7c55a06c8b88d5bcf22b51cf44a890\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: chex in /home/nahush/.local/lib/python3.10/site-packages (from jaxpruner==0.1) (0.1.7)\n",
            "Requirement already satisfied: flax in /home/nahush/.local/lib/python3.10/site-packages (from jaxpruner==0.1) (0.6.10)\n",
            "Requirement already satisfied: jax in /home/nahush/.local/lib/python3.10/site-packages (from jaxpruner==0.1) (0.4.11)\n",
            "Requirement already satisfied: jaxlib in /home/nahush/.local/lib/python3.10/site-packages (from jaxpruner==0.1) (0.4.11+cuda11.cudnn86)\n",
            "Requirement already satisfied: optax in /home/nahush/.local/lib/python3.10/site-packages (from jaxpruner==0.1) (0.1.5)\n",
            "Requirement already satisfied: numpy in /home/nahush/anaconda3/envs/jaxenv2/lib/python3.10/site-packages (from jaxpruner==0.1) (1.23.5)\n",
            "Requirement already satisfied: ml-collections in /home/nahush/.local/lib/python3.10/site-packages (from jaxpruner==0.1) (0.1.1)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /home/nahush/.local/lib/python3.10/site-packages (from chex->jaxpruner==0.1) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /home/nahush/.local/lib/python3.10/site-packages (from chex->jaxpruner==0.1) (4.6.3)\n",
            "Requirement already satisfied: absl-py>=0.9.0 in /home/nahush/.local/lib/python3.10/site-packages (from chex->jaxpruner==0.1) (1.4.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /home/nahush/.local/lib/python3.10/site-packages (from chex->jaxpruner==0.1) (0.1.8)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /home/nahush/.local/lib/python3.10/site-packages (from jax->jaxpruner==0.1) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /home/nahush/anaconda3/envs/jaxenv2/lib/python3.10/site-packages (from jax->jaxpruner==0.1) (1.10.1)\n",
            "Requirement already satisfied: opt-einsum in /home/nahush/.local/lib/python3.10/site-packages (from jax->jaxpruner==0.1) (3.3.0)\n",
            "Requirement already satisfied: rich>=11.1 in /home/nahush/.local/lib/python3.10/site-packages (from flax->jaxpruner==0.1) (13.4.1)\n",
            "Requirement already satisfied: msgpack in /home/nahush/anaconda3/envs/jaxenv2/lib/python3.10/site-packages (from flax->jaxpruner==0.1) (1.0.5)\n",
            "Requirement already satisfied: orbax-checkpoint in /home/nahush/.local/lib/python3.10/site-packages (from flax->jaxpruner==0.1) (0.2.4)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /home/nahush/anaconda3/envs/jaxenv2/lib/python3.10/site-packages (from flax->jaxpruner==0.1) (6.0)\n",
            "Requirement already satisfied: tensorstore in /home/nahush/.local/lib/python3.10/site-packages (from flax->jaxpruner==0.1) (0.1.37)\n",
            "Requirement already satisfied: contextlib2 in /home/nahush/.local/lib/python3.10/site-packages (from ml-collections->jaxpruner==0.1) (21.6.0)\n",
            "Requirement already satisfied: six in /home/nahush/anaconda3/envs/jaxenv2/lib/python3.10/site-packages (from ml-collections->jaxpruner==0.1) (1.16.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/nahush/.local/lib/python3.10/site-packages (from rich>=11.1->flax->jaxpruner==0.1) (2.15.1)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /home/nahush/.local/lib/python3.10/site-packages (from rich>=11.1->flax->jaxpruner==0.1) (2.2.0)\n",
            "Requirement already satisfied: cached_property in /home/nahush/.local/lib/python3.10/site-packages (from orbax-checkpoint->flax->jaxpruner==0.1) (1.5.2)\n",
            "Requirement already satisfied: importlib_resources in /home/nahush/.local/lib/python3.10/site-packages (from orbax-checkpoint->flax->jaxpruner==0.1) (5.12.0)\n",
            "Requirement already satisfied: nest_asyncio in /home/nahush/.local/lib/python3.10/site-packages (from orbax-checkpoint->flax->jaxpruner==0.1) (1.5.6)\n",
            "Requirement already satisfied: etils in /home/nahush/.local/lib/python3.10/site-packages (from orbax-checkpoint->flax->jaxpruner==0.1) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/nahush/.local/lib/python3.10/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=11.1->flax->jaxpruner==0.1) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install git+https://github.com/google-research/jaxpruner\n",
        "import jaxpruner\n",
        "import ml_collections"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gGi7zcRpp4ZL"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Data mean [0.49139968 0.48215841 0.44653091]\n",
            "Data std [0.24703223 0.24348513 0.26158784]\n"
          ]
        }
      ],
      "source": [
        "DATASET_PATH = \"./data\"\n",
        "train_dataset = CIFAR10(root=DATASET_PATH, train=True, download=True)\n",
        "DATA_MEANS = (train_dataset.data / 255.0).mean(axis=(0,1,2))\n",
        "DATA_STD = (train_dataset.data / 255.0).std(axis=(0,1,2))\n",
        "print(\"Data mean\", DATA_MEANS)\n",
        "print(\"Data std\", DATA_STD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def image_to_numpy(img):\n",
        "    img = np.array(img, dtype=np.float32)\n",
        "    img = (img / 255. - DATA_MEANS) / DATA_STD\n",
        "    return img\n",
        "\n",
        "# We need to stack the batch elements\n",
        "def numpy_collate(batch):\n",
        "    if isinstance(batch[0], np.ndarray):\n",
        "        return np.stack(batch)\n",
        "    elif isinstance(batch[0], (tuple,list)):\n",
        "        transposed = zip(*batch)\n",
        "        return [numpy_collate(samples) for samples in transposed]\n",
        "    else:\n",
        "        return np.array(batch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "test_transform = image_to_numpy\n",
        "# For training, we add some augmentation. Networks are too powerful and would overfit.\n",
        "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.RandomResizedCrop((32,32),scale=(0.8,1.0),ratio=(0.9,1.1)),\n",
        "                                      image_to_numpy\n",
        "                                     ])\n",
        "train_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=train_transform, download=True)\n",
        "val_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=test_transform, download=True)\n",
        "train_set, _ = torch.utils.data.random_split(train_dataset, [45000, 5000], generator=torch.Generator().manual_seed(42))\n",
        "_, val_set = torch.utils.data.random_split(val_dataset, [45000, 5000], generator=torch.Generator().manual_seed(42))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "test_set = CIFAR10(root=DATASET_PATH, train=False, transform=test_transform, download=True)\n",
        "train_loader = data.DataLoader(train_set,\n",
        "                               batch_size=128,\n",
        "                               shuffle=True,\n",
        "                               drop_last=True,\n",
        "                               collate_fn=numpy_collate,\n",
        "                               num_workers=8,\n",
        "                               persistent_workers=True)\n",
        "val_loader   = data.DataLoader(val_set,\n",
        "                               batch_size=128,\n",
        "                               shuffle=False,\n",
        "                               drop_last=False,\n",
        "                               collate_fn=numpy_collate,\n",
        "                               num_workers=4,\n",
        "                               persistent_workers=True)\n",
        "test_loader  = data.DataLoader(test_set,\n",
        "                               batch_size=128,\n",
        "                               shuffle=False,\n",
        "                               drop_last=False,\n",
        "                               collate_fn=numpy_collate,\n",
        "                               num_workers=4,\n",
        "                               persistent_workers=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "J7zXhAmXhfHt"
      },
      "outputs": [],
      "source": [
        "# def get_datasets():\n",
        "#   \"\"\"Load MNIST train and test datasets into memory.\"\"\"\n",
        "#   ds_builder = tfds.builder('mnist')\n",
        "#   ds_builder.download_and_prepare()\n",
        "#   train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train', batch_size=-1))\n",
        "#   test_ds = tfds.as_numpy(ds_builder.as_dataset(split='test', batch_size=-1))\n",
        "#   train_ds['image'] = jnp.float32(train_ds['image']) / 255.\n",
        "#   test_ds['image'] = jnp.float32(test_ds['image']) / 255.\n",
        "#   return train_ds, test_ds\n",
        "\n",
        "# # Helper functions for images.\n",
        "\n",
        "# def show_img(img, ax=None, title=None):\n",
        "#   \"\"\"Shows a single image.\"\"\"\n",
        "#   if ax is None:\n",
        "#     ax = plt.gca()\n",
        "#   ax.imshow(img[..., 0], cmap='gray')\n",
        "#   ax.set_xticks([])\n",
        "#   ax.set_yticks([])\n",
        "#   if title:\n",
        "#     ax.set_title(title)\n",
        "\n",
        "# def show_img_grid(imgs, titles):\n",
        "#   \"\"\"Shows a grid of images.\"\"\"\n",
        "#   n = int(np.ceil(len(imgs)**.5))\n",
        "#   _, axs = plt.subplots(n, n, figsize=(3 * n, 3 * n))\n",
        "#   for i, (img, title) in enumerate(zip(imgs, titles)):\n",
        "#     show_img(img, axs[i // n][i % n], title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRg0rNsJp4ZL",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Get datasets as dict of JAX arrays.\n",
        "train_ds, test_ds = get_datasets()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KqW8WP5bp4ZS"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KmG9pXPWh7sh"
      },
      "outputs": [],
      "source": [
        "import ml_collections\n",
        "\n",
        "# class CNN(nn.Module):\n",
        "#   \"\"\"A simple CNN model.\"\"\"\n",
        "\n",
        "#   @nn.compact\n",
        "#   def __call__(self, x):\n",
        "#     x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
        "#     x = nn.relu(x)\n",
        "#     x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
        "#     x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n",
        "#     x = nn.relu(x)\n",
        "#     x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
        "#     x = x.reshape((x.shape[0], -1))  # flatten\n",
        "#     x = nn.Dense(features=256)(x)\n",
        "#     x = nn.relu(x)\n",
        "#     x = nn.Dense(features=10)(x)\n",
        "#     return x\n",
        "from resnet import ResNet20 as res20\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def apply_model_train(state, images, labels):\n",
        "  \"\"\"Computes gradients, loss and accuracy for a single batch.\"\"\"\n",
        "  def loss_fn(params):\n",
        "  \n",
        "    logits,updates = state.apply_fn({'params': params, 'batch_stats': state.batch_stats}, images, train = True, mutable = ['batch_stats'])\n",
        "    \n",
        "    one_hot = jax.nn.one_hot(labels, 10)\n",
        "    loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=one_hot))\n",
        "    return loss, (logits, updates['batch_stats'])\n",
        "\n",
        "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "  (loss, (logits, new_batch_stats)), grads = grad_fn(state.params)\n",
        "  accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
        "  return grads, loss, accuracy, new_batch_stats\n",
        "@jax.jit\n",
        "def apply_model_test(state, images, labels,batch_stats):\n",
        "  \"\"\"Computes gradients, loss and accuracy for a single batch.\"\"\"\n",
        "  def loss_fn(params):\n",
        "  \n",
        "    logits = state.apply_fn({'params': params, 'batch_stats': batch_stats}, images, train = False)\n",
        "    one_hot = jax.nn.one_hot(labels, 10)\n",
        "    loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=one_hot))\n",
        "    return loss, logits\n",
        "\n",
        "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "  (loss, logits), grads = grad_fn(state.params)\n",
        "  accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
        "  return grads, loss, accuracy\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def update_model(state, grads, batch_stats):\n",
        "  # updates, new_opt_state = state.tx.update(\n",
        "  #     grads, state.opt_state, state.params)\n",
        "  # new_params = optax.apply_updates(state.params, updates)\n",
        "  state = state.apply_gradients(grads=grads, batch_stats=batch_stats)\n",
        "  return state\n",
        "def test_epoch(state,testloader, batch_stats):\n",
        "  epoch_loss = []\n",
        "  epoch_accuracy = []\n",
        "  for batch in tqdm(testloader,desc = 'Testing' , leave = False):\n",
        "    batch_images = batch[0]\n",
        "    batch_labels = batch[1]\n",
        "    _,loss,accuracy = apply_model_test(state,batch_images, batch_labels, batch_stats = batch_stats)\n",
        "    epoch_loss.append(loss)\n",
        "    epoch_accuracy.append(accuracy)\n",
        "  return np.mean(epoch_loss), np.mean(epoch_accuracy)\n",
        "\n",
        "def train_epoch(state, trainloader, sparsity_updater,batch_stats):\n",
        "  \"\"\"Train for a single epoch.\"\"\"\n",
        "  # train_ds_size = len(train_ds['image'])\n",
        "  # steps_per_epoch = train_ds_size // batch_size\n",
        "\n",
        "  # perms = jax.random.permutation(rng, len(train_ds['image']))\n",
        "  # perms = perms[:steps_per_epoch * batch_size]  # skip incomplete batch\n",
        "  # perms = perms.reshape((steps_per_epoch, batch_size))\n",
        "\n",
        "  epoch_loss = []\n",
        "  epoch_accuracy = []\n",
        "  is_ste = isinstance(sparsity_updater, (jaxpruner.SteMagnitudePruning,\n",
        "                                         jaxpruner.SteRandomPruning))\n",
        "  pre_op = jax.jit(sparsity_updater.pre_forward_update)\n",
        "  # for i, perm in enumerate(perms):\n",
        "  i = 1\n",
        "  for batch in tqdm(trainloader, desc = 'Training' , leave = False):\n",
        "    i+=1;\n",
        "    batch_images = batch[0]\n",
        "    batch_labels = batch[1]\n",
        "    # Following is only needed for STE.\n",
        "    new_params = pre_op(state.params, state.opt_state[2])\n",
        "    forward_state = state.replace(params=new_params)\n",
        "\n",
        "    grads, loss, accuracy, batch_stats = apply_model_train(forward_state, batch_images,\n",
        "                                        batch_labels)\n",
        "    state = update_model(state, grads, batch_stats)\n",
        "    post_params = sparsity_updater.post_gradient_update(\n",
        "        state.params, state.opt_state[2])\n",
        "    state = state.replace(params=post_params)\n",
        "    epoch_loss.append(loss)\n",
        "    epoch_accuracy.append(accuracy)\n",
        "    if i % 100 == 0:\n",
        "      if is_ste:\n",
        "        print(jaxpruner.summarize_sparsity(\n",
        "            new_params, only_total_sparsity=True))\n",
        "      else:\n",
        "        print(jaxpruner.summarize_sparsity(\n",
        "            state.params, only_total_sparsity=True))\n",
        "  train_loss = np.mean(epoch_loss)\n",
        "  train_accuracy = np.mean(epoch_accuracy)\n",
        "  return state, train_loss, train_accuracy\n",
        "\n",
        "\n",
        "def create_train_state(rng, config, num_epochs, num_steps_per_epoch):\n",
        "  \"\"\"Creates initial `TrainState`.\"\"\"\n",
        "  cnn = res20()\n",
        "  variables = cnn.init(rng, jnp.ones([1, 32,32, 3]),train = False)\n",
        "  params = variables['params']\n",
        "  batch_stats = variables['batch_stats']\n",
        "  sparsity_updater = jaxpruner.create_updater_from_config(config.sparsity_config)\n",
        "  # tx = optax.adam(config.learning_rate, config.momentum)\n",
        "  # tx = sparsity_updater.wrap_optax(tx)\n",
        "  opt_class = optax.sgd\n",
        "  lr_schedule = optax.piecewise_constant_schedule(\n",
        "            init_value=config.learning_rate,\n",
        "            boundaries_and_scales=\n",
        "                {int(num_steps_per_epoch*num_epochs*0.6): 0.1,\n",
        "                 int(num_steps_per_epoch*num_epochs*0.85): 0.1}\n",
        "        )\n",
        "        # Clip gradients at max value, and evt. apply weight decay\n",
        "  transf = [optax.clip(1.0)]\n",
        "  hparam_dict = { \"momentum\" : config.momentum, \"weight_decay\" : 1e-4}\n",
        "  if opt_class == optax.sgd :  # wd is integrated in adamw\n",
        "            transf.append(optax.add_decayed_weights(1e-4))\n",
        "  optimizer = optax.chain(\n",
        "            *transf,\n",
        "            sparsity_updater.wrap_optax(opt_class(lr_schedule))\n",
        "        )\n",
        "        # Initialize training state\n",
        "  # self.state = TrainState.create(apply_fn=self.model.apply,\n",
        "  #                                      params=self.init_params if self.state is None else self.state.params,\n",
        "  #                                      batch_stats=self.init_batch_stats if self.state is None else self.state.batch_stats,\n",
        "  #                                      tx=optimizer)\n",
        "\n",
        "\n",
        "  return TrainState.create(\n",
        "      apply_fn=cnn.apply, params=params, tx=optimizer, batch_stats = batch_stats), sparsity_updater\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "def train_and_evaluate(trainloader,testloader,config: ml_collections.ConfigDict\n",
        "                       ) -> train_state.TrainState:\n",
        "  \"\"\"Execute model training and evaluation loop.\n",
        "\n",
        "  Args:\n",
        "    config: Hyperparameter configuration for training and evaluation.\n",
        "    workdir: Directory where the tensorboard summaries are written to.\n",
        "\n",
        "  Returns:\n",
        "    The train state (which includes the `.params`).\n",
        "  \"\"\"\n",
        "  # train_ds, test_ds = get_datasets()\n",
        "  rng = jax.random.PRNGKey(0)\n",
        "\n",
        "  rng, init_rng = jax.random.split(rng)\n",
        "\n",
        "  state, sparsity_updater= create_train_state(init_rng, config, config.num_epochs, len(trainloader))\n",
        "\n",
        "  # for epoch in range(1, config.num_epochs + 1):\n",
        "  for epoch in tqdm(range(1,config.num_epochs+1)):\n",
        "    s_time = time.time()\n",
        "    rng, input_rng = jax.random.split(rng)\n",
        "    state, train_loss, train_accuracy = train_epoch(state, trainloader,\n",
        "                                                    sparsity_updater)\n",
        "    # Following is only needed for STE.\n",
        "    new_params = sparsity_updater.pre_forward_update(\n",
        "        state.params, state.opt_state[2])\n",
        "    forward_state = state.replace(params=new_params)\n",
        "      \n",
        "    test_loss, test_accuracy = test_epoch(forward_state, testloader,batch_stats)\n",
        "\n",
        "    print(\n",
        "        'epoch:% 3d, train_loss: %.4f, train_accuracy: %.2f, test_loss: %.4f, test_accuracy: %.2f, time: %.2f'\n",
        "        % (epoch, train_loss, train_accuracy * 100, test_loss,\n",
        "           test_accuracy * 100, time.time() - s_time))\n",
        "\n",
        "  return state"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "R5za4NrVw40K"
      },
      "source": [
        "# Jaxpruner API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Be-Bn7R6tsz7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('no_prune',\n",
              " 'magnitude',\n",
              " 'random',\n",
              " 'saliency',\n",
              " 'magnitude_ste',\n",
              " 'random_ste',\n",
              " 'global_magnitude',\n",
              " 'global_saliency',\n",
              " 'static_sparse',\n",
              " 'rigl',\n",
              " 'set')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "jaxpruner.ALGORITHMS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "XMVhTvc0tsM1"
      },
      "outputs": [],
      "source": [
        "config = ml_collections.ConfigDict()\n",
        "\n",
        "config.learning_rate = 0.01\n",
        "config.momentum = 0.9\n",
        "config.batch_size = 128\n",
        "config.num_epochs = 200 # 1 epoch is 468 steps for bs=128\n",
        "\n",
        "config.sparsity_config = ml_collections.ConfigDict()\n",
        "config.sparsity_config.algorithm = 'rigl'\n",
        "config.sparsity_config.update_freq = 10\n",
        "config.sparsity_config.update_end_step = 1000\n",
        "config.sparsity_config.update_start_step = 200\n",
        "config.sparsity_config.sparsity = 0.5\n",
        "config.sparsity_config.dist_type = 'erk'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "NihrDoDm2yAz"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RigL(scheduler=PolynomialSchedule(update_freq=10, update_start_step=200, update_end_step=1000, power=3), skip_gradients=True, is_sparse_gradients=True, sparsity_type=Unstructured(), sparsity_distribution_fn=functools.partial(<function erk at 0x7f5ee129ca60>, sparsity=0.5), rng_seed=Array([0, 8], dtype=uint32), use_packed_masks=False, eps=1e-05, drop_fraction_fn=<function cosine_decay_schedule.<locals>.schedule at 0x7f6086f17eb0>, is_debug=False)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "jaxpruner.create_updater_from_config(config.sparsity_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "TrainState.__init__() got an unexpected keyword argument 'batch_stats'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m state \u001b[39m=\u001b[39m train_and_evaluate(train_loader,test_loader,config)\n",
            "Cell \u001b[0;32mIn[12], line 169\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(trainloader, testloader, config)\u001b[0m\n\u001b[1;32m    165\u001b[0m rng \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mPRNGKey(\u001b[39m0\u001b[39m)\n\u001b[1;32m    167\u001b[0m rng, init_rng \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39msplit(rng)\n\u001b[0;32m--> 169\u001b[0m state, sparsity_updater\u001b[39m=\u001b[39m create_train_state(init_rng, config, config\u001b[39m.\u001b[39;49mnum_epochs, \u001b[39mlen\u001b[39;49m(trainloader))\n\u001b[1;32m    171\u001b[0m \u001b[39m# for epoch in range(1, config.num_epochs + 1):\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,config\u001b[39m.\u001b[39mnum_epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)):\n",
            "Cell \u001b[0;32mIn[12], line 147\u001b[0m, in \u001b[0;36mcreate_train_state\u001b[0;34m(rng, config, num_epochs, num_steps_per_epoch)\u001b[0m\n\u001b[1;32m    136\u001b[0m optimizer \u001b[39m=\u001b[39m optax\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    137\u001b[0m           \u001b[39m*\u001b[39mtransf,\n\u001b[1;32m    138\u001b[0m           sparsity_updater\u001b[39m.\u001b[39mwrap_optax(opt_class(lr_schedule))\n\u001b[1;32m    139\u001b[0m       )\n\u001b[1;32m    140\u001b[0m       \u001b[39m# Initialize training state\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39m# self.state = TrainState.create(apply_fn=self.model.apply,\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[39m#                                      params=self.init_params if self.state is None else self.state.params,\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39m#                                      batch_stats=self.init_batch_stats if self.state is None else self.state.batch_stats,\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[39m#                                      tx=optimizer)\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m \u001b[39mreturn\u001b[39;00m train_state\u001b[39m.\u001b[39;49mTrainState\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    148\u001b[0m     apply_fn\u001b[39m=\u001b[39;49mcnn\u001b[39m.\u001b[39;49mapply, params\u001b[39m=\u001b[39;49mparams, tx\u001b[39m=\u001b[39;49moptimizer, batch_stats \u001b[39m=\u001b[39;49m batch_stats), sparsity_updater\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/flax/training/train_state.py:87\u001b[0m, in \u001b[0;36mTrainState.create\u001b[0;34m(cls, apply_fn, params, tx, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Creates a new instance with `step=0` and initialized `opt_state`.\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m opt_state \u001b[39m=\u001b[39m tx\u001b[39m.\u001b[39minit(params)\n\u001b[0;32m---> 87\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\n\u001b[1;32m     88\u001b[0m     step\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     89\u001b[0m     apply_fn\u001b[39m=\u001b[39;49mapply_fn,\n\u001b[1;32m     90\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m     91\u001b[0m     tx\u001b[39m=\u001b[39;49mtx,\n\u001b[1;32m     92\u001b[0m     opt_state\u001b[39m=\u001b[39;49mopt_state,\n\u001b[1;32m     93\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     94\u001b[0m )\n",
            "\u001b[0;31mTypeError\u001b[0m: TrainState.__init__() got an unexpected keyword argument 'batch_stats'"
          ]
        }
      ],
      "source": [
        "state = train_and_evaluate(train_loader,test_loader,config)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "droLOMfCliZ8"
      },
      "source": [
        "# One Shot pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_prX9QDHmPa1"
      },
      "outputs": [],
      "source": [
        "config.sparsity_config.update_start_step = 0\n",
        "config.sparsity_config.update_end_step = 0\n",
        "config.sparsity_config.skip_gradients=True\n",
        "state = train_and_evaluate(config)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-mMq2qIuKk-U"
      },
      "source": [
        "# STE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hR114yDYLW8I"
      },
      "outputs": [],
      "source": [
        "# STE also supports gradual pruning schedules. \n",
        "# Here we train weights with sparse forward pass from the start.\n",
        "config.sparsity_config.algorithm = 'magnitude_ste'\n",
        "config.sparsity_config.sparsity = 0.95\n",
        "config.sparsity_config.update_end_step = 0\n",
        "config.sparsity_config.update_start_step = 0\n",
        "config.sparsity_config.dist_type = 'erk'\n",
        "state = train_and_evaluate(config)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7rPvHvyMouE"
      },
      "source": [
        "# Global Pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjTa5fSvMstH"
      },
      "outputs": [],
      "source": [
        "config.sparsity_config.algorithm = 'global_magnitude'\n",
        "config.sparsity_config.update_freq = 10\n",
        "config.sparsity_config.update_end_step = 1000\n",
        "config.sparsity_config.update_start_step = 200\n",
        "config.sparsity_config.sparsity = 0.95\n",
        "config.sparsity_config.dist_type = 'erk'\n",
        "\n",
        "state = train_and_evaluate(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVkUegpfNTBG"
      },
      "outputs": [],
      "source": [
        "jaxpruner.summarize_sparsity(state.opt_state.masks)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "z35houGrNaZR"
      },
      "source": [
        "# Dynamic Sparse Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo6DqrXXNc7J"
      },
      "outputs": [],
      "source": [
        "config.sparsity_config.algorithm = 'rigl'\n",
        "\n",
        "config.sparsity_config.update_freq = 10\n",
        "config.sparsity_config.update_end_step = 1000\n",
        "config.sparsity_config.update_start_step = 1\n",
        "config.sparsity_config.sparsity = 0.95\n",
        "config.sparsity_config.dist_type = 'erk'\n",
        "\n",
        "state = train_and_evaluate(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opcu8HK5NsNy"
      },
      "outputs": [],
      "source": [
        "config.sparsity_config.algorithm = 'set'\n",
        "state = train_and_evaluate(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNzNbM4GOF0J"
      },
      "outputs": [],
      "source": [
        "config.sparsity_config.algorithm = 'static_sparse'\n",
        "\n",
        "state = train_and_evaluate(config)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6rVpOIeDlvyJ"
      },
      "source": [
        "# Pruning After Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpZeZRXNlkQf"
      },
      "outputs": [],
      "source": [
        "config.sparsity_config.algorithm = 'no_prune'\n",
        "state = train_and_evaluate(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0q8fDRRnlqla"
      },
      "outputs": [],
      "source": [
        "config.sparsity_config.algorithm = 'magnitude'\n",
        "config.sparsity_config.sparsity = 0.9\n",
        "sparsity_updater = jaxpruner.create_updater_from_config(config.sparsity_config)\n",
        "pruned_params, _ = sparsity_updater.instant_sparsify(state.params)\n",
        "print(jaxpruner.summarize_sparsity(pruned_params, only_total_sparsity=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UMvlEqlwcee"
      },
      "outputs": [],
      "source": [
        "_, test_ds = get_datasets()\n",
        "pruned_state = state.replace(params=pruned_params)\n",
        "_, _, test_accuracy  = apply_model(pruned_state, test_ds['image'], test_ds['label'])\n",
        "print(test_accuracy*100)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EdHqmTATHuJx"
      },
      "source": [
        "# N:M sparsity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OrndhnkHwYb"
      },
      "outputs": [],
      "source": [
        "config.sparsity_config.algorithm = 'magnitude'\n",
        "config.sparsity_config.sparsity_type = 'nm_1,4'\n",
        "sparsity_updater = jaxpruner.create_updater_from_config(config.sparsity_config)\n",
        "pruned_params, masks = sparsity_updater.instant_sparsify(state.params)\n",
        "print(jaxpruner.summarize_sparsity(pruned_params, only_total_sparsity=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjv8gu_SJeZI"
      },
      "outputs": [],
      "source": [
        "masks['Dense_0']['kernel'][0][:16]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SL4ASPqHsdw"
      },
      "outputs": [],
      "source": [
        "_, test_ds = get_datasets()\n",
        "pruned_state = state.replace(params=pruned_params)\n",
        "_, _, test_accuracy  = apply_model(pruned_state, test_ds['image'], test_ds['label'])\n",
        "print(test_accuracy*100)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7tJ-Y0LSJ6b5"
      },
      "source": [
        "# Block Sparsity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l54RegrQJ50D"
      },
      "outputs": [],
      "source": [
        "config.sparsity_config.algorithm = 'magnitude'\n",
        "config.sparsity_config.sparsity = 0.7\n",
        "config.sparsity_config.sparsity_type = 'block_2,2'\n",
        "sparsity_updater = jaxpruner.create_updater_from_config(config.sparsity_config)\n",
        "pruned_params, masks = sparsity_updater.instant_sparsify(state.params)\n",
        "print(jaxpruner.summarize_sparsity(pruned_params, only_total_sparsity=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0vONI48KEaG"
      },
      "outputs": [],
      "source": [
        "masks['Dense_0']['kernel'][:4, :16]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrNGCVeEJ_Fx"
      },
      "outputs": [],
      "source": [
        "_, test_ds = get_datasets()\n",
        "pruned_state = state.replace(params=pruned_params)\n",
        "_, _, test_accuracy  = apply_model(pruned_state, test_ds['image'], test_ds['label'])\n",
        "print(test_accuracy*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:GlobalAsyncCheckpointManager is not imported correctly. Checkpointing of GlobalDeviceArrays will not be available.To use the feature, install tensorstore.\n"
          ]
        }
      ],
      "source": [
        "from typing import Any\n",
        "from collections import defaultdict\n",
        "from flax.training import train_state, checkpoints\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TrainerModule:\n",
        "\n",
        "    def __init__(self,\n",
        "                 model_name : str,\n",
        "                 model_class : nn.Module,\n",
        "                 model_hparams : dict,\n",
        "                 optimizer_name : str,\n",
        "                 optimizer_hparams : dict,\n",
        "                 exmp_imgs : Any,\n",
        "                 seed=42):\n",
        "        \"\"\"\n",
        "        Module for summarizing all training functionalities for classification on CIFAR10.\n",
        "\n",
        "        Inputs:\n",
        "            model_name - String of the class name, used for logging and saving\n",
        "            model_class - Class implementing the neural network\n",
        "            model_hparams - Hyperparameters of the model, used as input to model constructor\n",
        "            optimizer_name - String of the optimizer name, supporting ['sgd', 'adam', 'adamw']\n",
        "            optimizer_hparams - Hyperparameters of the optimizer, including learning rate as 'lr'\n",
        "            exmp_imgs - Example imgs, used as input to initialize the model\n",
        "            seed - Seed to use in the model initialization\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.model_name = model_name\n",
        "        self.model_class = model_class\n",
        "        self.model_hparams = model_hparams\n",
        "        self.optimizer_name = optimizer_name\n",
        "        self.optimizer_hparams = optimizer_hparams\n",
        "        self.seed = seed\n",
        "        # Create empty model. Note: no parameters yet\n",
        "        self.model = res20()\n",
        "        # Prepare logging\n",
        "        self.log_dir = os.path.join('./', self.model_name)\n",
        "        self.logger = SummaryWriter(log_dir=self.log_dir)\n",
        "        # Create jitted training and eval functions\n",
        "        self.create_functions()\n",
        "        # Initialize model\n",
        "        self.init_model(exmp_imgs)\n",
        "\n",
        "    def create_functions(self):\n",
        "        # Function to calculate the classification loss and accuracy for a model\n",
        "        def calculate_loss(params, batch_stats, batch, train):\n",
        "            imgs, labels = batch\n",
        "            # Run model. During training, we need to update the BatchNorm statistics.\n",
        "            outs = self.model.apply({'params': params, 'batch_stats': batch_stats},\n",
        "                                    imgs,\n",
        "                                    train=train,\n",
        "                                    mutable=['batch_stats'] if train else False)\n",
        "            logits, new_model_state = outs if train else (outs, None)\n",
        "            loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n",
        "            acc = (logits.argmax(axis=-1) == labels).mean()\n",
        "            return loss, (acc, new_model_state)\n",
        "        # Training function\n",
        "        def train_step(state, batch):\n",
        "            loss_fn = lambda params: calculate_loss(params, state.batch_stats, batch, train=True)\n",
        "            # Get loss, gradients for loss, and other outputs of loss function\n",
        "            ret, grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
        "            loss, acc, new_model_state = ret[0], *ret[1]\n",
        "            # Update parameters and batch statistics\n",
        "            state = state.apply_gradients(grads=grads, batch_stats=new_model_state['batch_stats'])\n",
        "            return state, loss, acc\n",
        "        # Eval function\n",
        "        def eval_step(state, batch):\n",
        "            # Return the accuracy for a single batch\n",
        "            _, (acc, _) = calculate_loss(state.params, state.batch_stats, batch, train=False)\n",
        "            return acc\n",
        "        # jit for efficiency\n",
        "        self.train_step = jax.jit(train_step)\n",
        "        self.eval_step = jax.jit(eval_step)\n",
        "\n",
        "    def init_model(self, exmp_imgs):\n",
        "        # Initialize model\n",
        "        init_rng = jax.random.PRNGKey(self.seed)\n",
        "        variables = self.model.init(init_rng, exmp_imgs, train=True)\n",
        "        self.init_params, self.init_batch_stats = variables['params'], variables['batch_stats']\n",
        "        self.state = None\n",
        "\n",
        "    def init_optimizer(self, num_epochs, num_steps_per_epoch):\n",
        "        # Initialize learning rate schedule and optimizer\n",
        "        if self.optimizer_name.lower() == 'adam':\n",
        "            opt_class = optax.adam\n",
        "        elif self.optimizer_name.lower() == 'adamw':\n",
        "            opt_class = optax.adamw\n",
        "        elif self.optimizer_name.lower() == 'sgd':\n",
        "            opt_class = optax.sgd\n",
        "        else:\n",
        "            assert False, f'Unknown optimizer \"{opt_class}\"'\n",
        "        # We decrease the learning rate by a factor of 0.1 after 60% and 85% of the training\n",
        "        lr_schedule = optax.piecewise_constant_schedule(\n",
        "            init_value=self.optimizer_hparams.pop('lr'),\n",
        "            boundaries_and_scales=\n",
        "                {int(num_steps_per_epoch*num_epochs*0.6): 0.1,\n",
        "                 int(num_steps_per_epoch*num_epochs*0.85): 0.1}\n",
        "        )\n",
        "        # Clip gradients at max value, and evt. apply weight decay\n",
        "        # transf = [optax.clip(1.0)]\n",
        "        if opt_class == optax.sgd and 'weight_decay' in self.optimizer_hparams:  # wd is integrated in adamw\n",
        "            transf.append(optax.add_decayed_weights(self.optimizer_hparams.pop('weight_decay')))\n",
        "        optimizer = optax.chain(\n",
        "            *transf,\n",
        "            opt_class(lr_schedule, **self.optimizer_hparams)\n",
        "        )\n",
        "        # Initialize training state\n",
        "        self.state = TrainState.create(apply_fn=self.model.apply,\n",
        "                                       params=self.init_params if self.state is None else self.state.params,\n",
        "                                       batch_stats=self.init_batch_stats if self.state is None else self.state.batch_stats,\n",
        "                                       tx=optimizer)\n",
        "\n",
        "    def train_model(self, train_loader, val_loader, num_epochs=200):\n",
        "        # Train model for defined number of epochs\n",
        "        # We first need to create optimizer and the scheduler for the given number of epochs\n",
        "        self.init_optimizer(num_epochs, len(train_loader))\n",
        "        # Track best eval accuracy\n",
        "        best_eval = 0.0\n",
        "        for epoch_idx in tqdm(range(1, num_epochs+1)):\n",
        "            self.train_epoch(train_loader, epoch=epoch_idx)\n",
        "            if epoch_idx % 2 == 0:\n",
        "                eval_acc = self.eval_model(val_loader)\n",
        "                self.logger.add_scalar('val/acc', eval_acc, global_step=epoch_idx)\n",
        "                if eval_acc >= best_eval:\n",
        "                    best_eval = eval_acc\n",
        "                    self.save_model(step=epoch_idx)\n",
        "                self.logger.flush()\n",
        "\n",
        "    def train_epoch(self, train_loader, epoch):\n",
        "        # Train model for one epoch, and log avg loss and accuracy\n",
        "        metrics = defaultdict(list)\n",
        "        for batch in tqdm(train_loader, desc='Training', leave=False):\n",
        "            self.state, loss, acc = self.train_step(self.state, batch)\n",
        "            metrics['loss'].append(loss)\n",
        "            metrics['acc'].append(acc)\n",
        "        for key in metrics:\n",
        "            avg_val = np.stack(jax.device_get(metrics[key])).mean()\n",
        "            self.logger.add_scalar('train/'+key, avg_val, global_step=epoch)\n",
        "\n",
        "    def eval_model(self, data_loader):\n",
        "        # Test model on all images of a data loader and return avg loss\n",
        "        correct_class, count = 0, 0\n",
        "        for batch in data_loader:\n",
        "            acc = self.eval_step(self.state, batch)\n",
        "            correct_class += acc * batch[0].shape[0]\n",
        "            count += batch[0].shape[0]\n",
        "        eval_acc = (correct_class / count).item()\n",
        "        return eval_acc\n",
        "\n",
        "    def save_model(self, step=0):\n",
        "        # Save current model at certain training iteration\n",
        "        checkpoints.save_checkpoint(ckpt_dir=self.log_dir,\n",
        "                                    target={'params': self.state.params,\n",
        "                                            'batch_stats': self.state.batch_stats},\n",
        "                                    step=step,\n",
        "                                   overwrite=True)\n",
        "\n",
        "    def load_model(self, pretrained=False):\n",
        "        # Load model. We use different checkpoint for pretrained models\n",
        "        if not pretrained:\n",
        "            state_dict = checkpoints.restore_checkpoint(ckpt_dir=self.log_dir, target=None)\n",
        "        else:\n",
        "            state_dict = checkpoints.restore_checkpoint(ckpt_dir=os.path.join('./', f'{self.model_name}.ckpt'), target=None)\n",
        "        self.state = TrainState.create(apply_fn=self.model.apply,\n",
        "                                       params=state_dict['params'],\n",
        "                                       batch_stats=state_dict['batch_stats'],\n",
        "                                       tx=self.state.tx if self.state else optax.sgd(0.1)   # Default optimizer\n",
        "                                      )\n",
        "\n",
        "    def checkpoint_exists(self):\n",
        "        # Check whether a pretrained model exist for this autoencoder\n",
        "        return os.path.isfile(os.path.join('./', f'{self.model_name}.ckpt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TrainState(train_state.TrainState):\n",
        "    # A simple extension of TrainState to also include batch statistics\n",
        "    batch_stats: Any"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_classifier(*args, num_epochs=200, **kwargs):\n",
        "    # Create a trainer module with specified hyperparameters\n",
        "    trainer = TrainerModule(*args, **kwargs)\n",
        "    if not trainer.checkpoint_exists():  # Skip training if pretrained model exists\n",
        "        trainer.train_model(train_loader, val_loader, num_epochs=num_epochs)\n",
        "        trainer.load_model()\n",
        "    else:\n",
        "        trainer.load_model(pretrained=True)\n",
        "    # Test trained model\n",
        "    val_acc = trainer.eval_model(val_loader)\n",
        "    test_acc = trainer.eval_model(test_loader)\n",
        "    return trainer, {'val': val_acc, 'test': test_acc}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resnet_kernel_init = nn.initializers.variance_scaling(2.0, mode='fan_out', distribution='normal')\n",
        "\n",
        "class ResNetBlock(nn.Module):\n",
        "    act_fn : callable  # Activation function\n",
        "    c_out : int   # Output feature size\n",
        "    subsample : bool = False  # If True, we apply a stride inside F\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, train=True):\n",
        "        # Network representing F\n",
        "        z = nn.Conv(self.c_out, kernel_size=(3, 3),\n",
        "                    strides=(1, 1) if not self.subsample else (2, 2),\n",
        "                    kernel_init=resnet_kernel_init,\n",
        "                    use_bias=False)(x)\n",
        "        z = nn.BatchNorm()(z, use_running_average=not train)\n",
        "        z = self.act_fn(z)\n",
        "        z = nn.Conv(self.c_out, kernel_size=(3, 3),\n",
        "                    kernel_init=resnet_kernel_init,\n",
        "                    use_bias=False)(z)\n",
        "        z = nn.BatchNorm()(z, use_running_average=not train)\n",
        "\n",
        "        if self.subsample:\n",
        "            x = nn.Conv(self.c_out, kernel_size=(1, 1), strides=(2, 2), kernel_init=resnet_kernel_init)(x)\n",
        "\n",
        "        x_out = self.act_fn(z + x)\n",
        "        return x_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResNet(nn.Module):\n",
        "    num_classes : int\n",
        "    act_fn : callable\n",
        "    block_class : nn.Module\n",
        "    num_blocks : tuple = (3, 3, 3)\n",
        "    c_hidden : tuple = (16, 32, 64)\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, train=True):\n",
        "        # A first convolution on the original image to scale up the channel size\n",
        "        x = nn.Conv(self.c_hidden[0], kernel_size=(3, 3), kernel_init=resnet_kernel_init, use_bias=False)(x)\n",
        "        if self.block_class == ResNetBlock:  # If pre-activation block, we do not apply non-linearities yet\n",
        "            x = nn.BatchNorm()(x, use_running_average=not train)\n",
        "            x = self.act_fn(x)\n",
        "\n",
        "        # Creating the ResNet blocks\n",
        "        for block_idx, block_count in enumerate(self.num_blocks):\n",
        "            for bc in range(block_count):\n",
        "                # Subsample the first block of each group, except the very first one.\n",
        "                subsample = (bc == 0 and block_idx > 0)\n",
        "                # ResNet block\n",
        "                x = self.block_class(c_out=self.c_hidden[block_idx],\n",
        "                                     act_fn=self.act_fn,\n",
        "                                     subsample=subsample)(x, train=train)\n",
        "\n",
        "        # Mapping to classification output\n",
        "        x = x.mean(axis=(1, 2))\n",
        "        x = nn.Dense(self.num_classes)(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resnet_trainer, resnet_results = train_classifier(model_name=\"ResNet\",\n",
        "                                                  model_class=ResNet,\n",
        "                                                  model_hparams={\"num_classes\": 10,\n",
        "                                                                 \"c_hidden\": (16, 32, 64),\n",
        "                                                                 \"num_blocks\": (3, 3, 3),\n",
        "                                                                 \"act_fn\": nn.relu,\n",
        "                                                                 \"block_class\": ResNetBlock},\n",
        "                                                  optimizer_name=\"SGD\",\n",
        "                                                  optimizer_hparams={\"lr\": 0.1,\n",
        "                                                                     \"momentum\": 0.9,\n",
        "                                                                     \"weight_decay\": 1e-4},\n",
        "                                                  exmp_imgs=jax.device_put(\n",
        "                                                      next(iter(train_loader))[0]),\n",
        "                                                  num_epochs=200)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resnet_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "last_runtime": {
        "build_target": "//learning/grp/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
